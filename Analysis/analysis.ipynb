{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google Ads Strategic Brief Analysis\n",
        "\n",
        "This notebook analyzes Google Trends data to create a strategic brief for new Google Ads campaigns in Park City.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set plot style\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the parent directory of the notebook\n",
        "base_path = Path.cwd()\n",
        "data_path = base_path\n",
        "\n",
        "# List of theme directories\n",
        "themes = [\n",
        "    'Deer Valley East Real Estate',\n",
        "    'Deer Valley Real Estate',\n",
        "    'Glenwild',\n",
        "    'Heber Utah Real Estate',\n",
        "    'Kamas Real Estate',\n",
        "    'Park City Real Estate',\n",
        "    'Promontory Park City /',\n",
        "    'Red Ledges Real Estate',\n",
        "    'Ski in Ski Out Home for Sale',\n",
        "    'Victory Ranch Real Esate'\n",
        "]\n",
        "\n",
        "def clean_theme_name(theme):\n",
        "    theme = theme.replace(' /', '').replace(' Esate', ' Estate')\n",
        "    return theme\n",
        "\n",
        "# Load multiTimeline data\n",
        "timeline_dfs = []\n",
        "for theme in themes:\n",
        "    files = glob.glob(str(data_path / theme / 'multiTimeline*.csv'))\n",
        "    for file in files:\n",
        "        try:\n",
        "            df = pd.read_csv(file, skiprows=2)\n",
        "            df.columns = ['Week', 'Interest']\n",
        "            df['Theme'] = clean_theme_name(theme)\n",
        "            # Handle non-numeric interest values like '<1'\n",
        "            df['Interest'] = pd.to_numeric(df['Interest'], errors='coerce').fillna(0.5)\n",
        "            df['Week'] = pd.to_datetime(df['Week'].str.split(' - ').str[0])\n",
        "            timeline_dfs.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not read {file}: {e}\")\n",
        "\n",
        "if timeline_dfs:\n",
        "    timeline_df = pd.concat(timeline_dfs, ignore_index=True)\n",
        "    print(\"Timeline data loaded successfully.\")\n",
        "    print(timeline_df.head())\n",
        "    print(f\"Date range: {timeline_df['Week'].min()} to {timeline_df['Week'].max()}\")\n",
        "else:\n",
        "    print(\"No timeline data loaded.\")\n",
        "\n",
        "# Load geoMap data\n",
        "geomap_dfs = []\n",
        "for theme in themes:\n",
        "    files = glob.glob(str(data_path / theme / 'geoMap*.csv'))\n",
        "    for file in files:\n",
        "        try:\n",
        "            df = pd.read_csv(file, skiprows=2)\n",
        "            df.columns = ['Metro', 'Interest']\n",
        "            df['Theme'] = clean_theme_name(theme)\n",
        "            # Handle non-numeric interest values\n",
        "            df['Interest'] = pd.to_numeric(df['Interest'], errors='coerce').fillna(0)\n",
        "            geomap_dfs.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not read {file}: {e}\")\n",
        "\n",
        "if geomap_dfs:\n",
        "    geomap_df = pd.concat(geomap_dfs, ignore_index=True)\n",
        "    # Some metro data might be at the country or city level. We only want metro.\n",
        "    geomap_df = geomap_df[geomap_df['Metro'].str.contains('Metro')]\n",
        "    geomap_df['Metro'] = geomap_df['Metro'].str.replace(' Metro', '')\n",
        "    print(\"\\nGeoMap data loaded successfully.\")\n",
        "    print(geomap_df.head())\n",
        "else:\n",
        "    print(\"No GeoMap data loaded.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Campaign & Ad Group Clustering\n",
        "\n",
        "Here we'll identify themes that behave similarly and would make sense to group together in a Google Ads campaign. We'll base this grouping on similar seasonal trends and overlapping geographic interest.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -- Seasonality Clustering --\n",
        "\n",
        "# Resample weekly data to monthly to smooth it out\n",
        "timeline_df['Month'] = timeline_df['Week'].dt.to_period('M')\n",
        "monthly_interest = timeline_df.groupby(['Theme', 'Month'])['Interest'].mean().reset_index()\n",
        "monthly_interest['Month'] = monthly_interest['Month'].dt.month\n",
        "\n",
        "# Pivot table for seasonality\n",
        "seasonal_pivot = monthly_interest.pivot_table(index='Theme', columns='Month', values='Interest').fillna(0)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "seasonal_scaled = scaler.fit_transform(seasonal_pivot)\n",
        "\n",
        "# Elbow method to find optimal k\n",
        "sse = {}\n",
        "for k in range(1, 10):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10).fit(seasonal_scaled)\n",
        "    sse[k] = kmeans.inertia_\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(list(sse.keys()), list(sse.values()))\n",
        "plt.xlabel(\"Number of cluster\")\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.title(\"Elbow Method for Seasonality Clustering\")\n",
        "plt.show()\n",
        "\n",
        "# Based on the elbow plot, let's choose k=3 (assuming a visible elbow)\n",
        "k_seasonal = 3\n",
        "kmeans_seasonal = KMeans(n_clusters=k_seasonal, random_state=42, n_init=10)\n",
        "seasonal_pivot['Seasonal_Cluster'] = kmeans_seasonal.fit_predict(seasonal_scaled)\n",
        "\n",
        "print(\"Seasonal Clusters:\")\n",
        "print(seasonal_pivot.groupby('Seasonal_Cluster').groups)\n",
        "\n",
        "\n",
        "# -- Geographic Clustering --\n",
        "\n",
        "# Pivot table for geographic interest\n",
        "geo_pivot = geomap_df.pivot_table(index='Theme', columns='Metro', values='Interest').fillna(0)\n",
        "\n",
        "# Normalize\n",
        "geo_scaled = scaler.fit_transform(geo_pivot)\n",
        "\n",
        "# Elbow method\n",
        "sse_geo = {}\n",
        "for k in range(1, 10):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10).fit(geo_scaled)\n",
        "    sse_geo[k] = kmeans.inertia_\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(list(sse_geo.keys()), list(sse_geo.values()))\n",
        "plt.xlabel(\"Number of cluster\")\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.title(\"Elbow Method for Geographic Clustering\")\n",
        "plt.show()\n",
        "\n",
        "# Based on the elbow plot, let's choose k=3 (assuming a visible elbow)\n",
        "k_geo = 3\n",
        "kmeans_geo = KMeans(n_clusters=k_geo, random_state=42, n_init=10)\n",
        "geo_pivot['Geo_Cluster'] = kmeans_geo.fit_predict(geo_scaled)\n",
        "\n",
        "print(\"\\nGeographic Clusters:\")\n",
        "print(geo_pivot.groupby('Geo_Cluster').groups)\n",
        "\n",
        "# -- Combined Analysis & Recommendation --\n",
        "\n",
        "# Combine cluster results\n",
        "clusters = pd.DataFrame({\n",
        "    'Seasonal_Cluster': seasonal_pivot['Seasonal_Cluster'],\n",
        "    'Geo_Cluster': geo_pivot['Geo_Cluster']\n",
        "})\n",
        "print(\"\\nCombined Cluster Analysis:\")\n",
        "print(clusters)\n",
        "\n",
        "# Generate Strategic Recommendation\n",
        "print(\"\\n--- Strategic Recommendation: Campaign Structure ---\")\n",
        "print(\"Based on the clustering, we can propose the following campaign structure:\")\n",
        "\n",
        "for i in range(k_seasonal):\n",
        "    for j in range(k_geo):\n",
        "        cluster_group = clusters[(clusters['Seasonal_Cluster'] == i) & (clusters['Geo_Cluster'] == j)]\n",
        "        if not cluster_group.empty:\n",
        "            print(f\"\\nCampaign: Seasonal Cluster {i} / Geo Cluster {j}\")\n",
        "            print(\"Themes:\", \", \".join(cluster_group.index))\n",
        "            print(\"Characteristics: These themes share similar seasonality and geographic targeting.\")\n",
        "            print(\"Recommendation: Group these into a single campaign to manage budget and targeting efficiently. Ad groups can be created for each individual theme within this campaign.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Market Prioritization\n",
        "\n",
        "Here we create two \"Top 5\" lists: by average monthly search volume and by year-over-year growth. This will help us decide where to focus our initial budget.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -- a) Top 5 themes by average monthly search Volume --\n",
        "\n",
        "avg_volume = timeline_df.groupby('Theme')['Interest'].mean().sort_values(ascending=False)\n",
        "top5_volume = avg_volume.head(5)\n",
        "\n",
        "print(\"--- Top 5 Themes by Average Search Volume ---\")\n",
        "print(top5_volume)\n",
        "print(\"\\\\n\")\n",
        "\n",
        "\n",
        "# -- b) Top 5 themes by year-over-year Growth (CAGR) --\n",
        "\n",
        "timeline_df['Year'] = timeline_df['Week'].dt.year\n",
        "yearly_interest = timeline_df.groupby(['Theme', 'Year'])['Interest'].mean().reset_index()\n",
        "\n",
        "def calculate_cagr(df):\n",
        "    df = df.sort_values('Year')\n",
        "    start_row = df.iloc[0]\n",
        "    end_row = df.iloc[-1]\n",
        "    \n",
        "    start_value = start_row['Interest']\n",
        "    end_value = end_row['Interest']\n",
        "    num_years = end_row['Year'] - start_row['Year']\n",
        "    \n",
        "    if num_years > 0 and start_value > 0:\n",
        "        cagr = ((end_value / start_value) ** (1 / num_years)) - 1\n",
        "        return cagr\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "cagr_results = yearly_interest.groupby('Theme').apply(calculate_cagr).dropna()\n",
        "top5_growth = cagr_results.sort_values(ascending=False).head(5)\n",
        "\n",
        "print(\"--- Top 5 Themes by Year-over-Year Growth (CAGR) ---\")\n",
        "print(top5_growth)\n",
        "print(\"\\\\n\")\n",
        "\n",
        "\n",
        "# -- Strategic Recommendation --\n",
        "\n",
        "print(\"--- Strategic Recommendation: Budget Allocation ---\")\n",
        "print(\"To maximize immediate impact, we should allocate a significant portion of our initial budget to the **Top 5 themes by volume**. These are established markets with high, consistent search interest. A suggested starting split is 60-70% of the budget here.\")\n",
        "print(f\"High-Volume Themes: {', '.join(top5_volume.index)}\")\n",
        "print(\"\\\\n\")\n",
        "print(\"To capture future growth and gain a competitive advantage, we should invest the remaining budget (30-40%) in the **Top 5 themes by growth**. These are emerging opportunities that could become major markets. While the volume is lower now, early investment can establish brand leadership.\")\n",
        "print(f\"High-Growth Themes: {', '.join(top5_growth.index)}\")\n",
        "print(\"\\\\n\")\n",
        "print(\"This balanced approach ensures we are competing in the largest current markets while also investing in the future of Park City real estate search trends.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Detailed Thematic Analysis\n",
        "\n",
        "For each of the themes, we provide a consistent summary card with information on peak seasonality and top metro areas to inform ad scheduling and location targeting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import calendar\n",
        "\n",
        "all_themes = sorted(timeline_df['Theme'].unique())\n",
        "\n",
        "print(\"--- Detailed Thematic Analysis ---\")\n",
        "\n",
        "for theme in all_themes:\n",
        "    print(f\"\\\\n### Theme: {theme}\")\n",
        "    \n",
        "    # Peak Seasonality\n",
        "    theme_timeline = timeline_df[timeline_df['Theme'] == theme]\n",
        "    # Use week of year for more specific seasonality\n",
        "    theme_timeline['Week_of_Year'] = theme_timeline['Week'].dt.isocalendar().week\n",
        "    peak_week = theme_timeline.groupby('Week_of_Year')['Interest'].mean().idxmax()\n",
        "    \n",
        "    # Top Metro Area\n",
        "    theme_geomap = geomap_df[geomap_df['Theme'] == theme]\n",
        "    if not theme_geomap.empty:\n",
        "        top_metro = theme_geomap.loc[theme_geomap['Interest'].idxmax()]\n",
        "        top_metro_name = top_metro['Metro']\n",
        "    else:\n",
        "        top_metro_name = \"N/A\"\n",
        "\n",
        "    print(f\"*   **Peak Seasonality:** Week {peak_week}\")\n",
        "    print(f\"*   **Top Metro Area:** {top_metro_name}\")\n",
        "    \n",
        "    # Strategic Recommendation\n",
        "    print(\"*   **Strategic Recommendation:**\")\n",
        "    print(f\"    *   **Ad Scheduling:** Concentrate ad spend during the peak season around Week {peak_week}. Consider launching awareness campaigns a few weeks prior to build momentum.\")\n",
        "    print(f\"    *   **Location Targeting:** Prioritize bidding and budget for the {top_metro_name} metro area. Use this location for targeted ad copy and potentially unique landing pages.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Geographic Deep Dive: Top Metro Areas\n",
        "\n",
        "Here we identify the top metro areas with the highest overall search volume and the most popular themes within those metros. This will help us create highly relevant, geo-targeted ad copy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -- a) Identify the Top 5 metro areas with the highest search volume overall --\n",
        "\n",
        "metro_total_interest = geomap_df.groupby('Metro')['Interest'].sum().sort_values(ascending=False)\n",
        "top5_metros = metro_total_interest.head(5)\n",
        "\n",
        "print(\"--- Top 5 Metro Areas by Overall Search Volume ---\")\n",
        "print(top5_metros)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# -- b) For each of these Top 5 metro areas, list the top 3 most popular search themes --\n",
        "\n",
        "print(\"--- Top Themes within Top 5 Metro Areas ---\")\n",
        "for metro in top5_metros.index:\n",
        "    print(f\"\\n### Metro: {metro}\")\n",
        "    metro_themes = geomap_df[geomap_df['Metro'] == metro]\n",
        "    top3_themes = metro_themes.sort_values('Interest', ascending=False).head(3)\n",
        "    for i, row in top3_themes.iterrows():\n",
        "        print(f\"*   {row['Theme']} (Interest: {row['Interest']})\")\n",
        "\n",
        "# -- Strategic Recommendation --\n",
        "\n",
        "print(\"\\n--- Strategic Recommendation: Geo-Targeted Ad Copy ---\")\n",
        "print(\"This data allows for highly specific and resonant ad copy. We can tailor our messaging to what users in a specific metro area are most interested in.\")\n",
        "print(\"\\n**Examples:**\")\n",
        "for metro in top5_metros.index:\n",
        "    top_theme = geomap_df[geomap_df['Metro'] == metro].sort_values('Interest', ascending=False).iloc[0]['Theme']\n",
        "    metro_name_parts = metro.split('-')\n",
        "    city_name = metro_name_parts[0]\n",
        "    if 'CA' in metro:\n",
        "        state_name = \"California\"\n",
        "    elif 'NY' in metro:\n",
        "        state_name = \"New York\"\n",
        "    elif 'IL' in metro:\n",
        "        state_name = \"Illinois\"\n",
        "    elif 'TX' in metro:\n",
        "        state_name = \"Texas\"\n",
        "    else:\n",
        "        state_name = \"local\"\n",
        "    \n",
        "    print(f\"*   **For {metro}:** Since '{top_theme}' is the top search, an ad could read: '{city_name}, looking for your dream {top_theme.lower()}? Explore exclusive Park City listings for {state_name} buyers.'\")\n",
        "\n",
        "print(\"\\nThis level of personalization can significantly increase click-through rates and lead quality.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Final Report Generation\n",
        "\n",
        "This final step will consolidate all the analysis and recommendations into a clean, well-structured markdown report. I will execute the notebook and then format the output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is a placeholder cell to indicate the end of the analysis.\n",
        "# The next step is to run the notebook and generate the markdown report from the output.\n",
        "print(\"Analysis notebook complete. Ready to generate report.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
