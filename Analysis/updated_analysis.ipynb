{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Updated Google Ads Strategic Brief Analysis\n",
        "\n",
        "This notebook analyzes Google Trends data across multiple timeframes (1-year, 2-year, and 5-year) to create an updated strategic brief for new Google Ads campaigns in Park City.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set plot style\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data Loading ---\n",
        "\n",
        "base_path = Path.cwd()\n",
        "data_path = base_path\n",
        "\n",
        "themes = [\n",
        "    'Deer Valley East Real Estate', 'Deer Valley Real Estate', 'Glenwild',\n",
        "    'Heber Utah Real Estate', 'Kamas Real Estate', 'Park City Real Estate',\n",
        "    'Promontory Park City /', 'Red Ledges Real Estate', \n",
        "    'Ski in Ski Out Home for Sale', 'Victory Ranch Real Esate'\n",
        "]\n",
        "timeframes = ['1 year', '2 year', '5 year']\n",
        "\n",
        "def clean_theme_name(theme):\n",
        "    return theme.replace(' /', '').replace(' Esate', ' Estate')\n",
        "\n",
        "# Load multiTimeline data\n",
        "timeline_dfs = []\n",
        "for theme in themes:\n",
        "    for timeframe in timeframes:\n",
        "        files = glob.glob(str(data_path / theme / timeframe / 'multiTimeline*.csv'))\n",
        "        for file in files:\n",
        "            try:\n",
        "                df = pd.read_csv(file, skiprows=2)\n",
        "                df.columns = ['Week', 'Interest']\n",
        "                df['Theme'] = clean_theme_name(theme)\n",
        "                df['Timeframe'] = timeframe\n",
        "                df['Interest'] = pd.to_numeric(df['Interest'], errors='coerce').fillna(0.5)\n",
        "                df['Week'] = pd.to_datetime(df['Week'].str.split(' - ').str[0])\n",
        "                timeline_dfs.append(df)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not read {file}: {e}\")\n",
        "\n",
        "if timeline_dfs:\n",
        "    master_timeline_df = pd.concat(timeline_dfs, ignore_index=True).drop_duplicates()\n",
        "    print(\"Timeline data loaded successfully.\")\n",
        "    print(f\"Loaded {len(master_timeline_df)} rows of timeline data.\")\n",
        "else:\n",
        "    print(\"No timeline data loaded.\")\n",
        "    master_timeline_df = pd.DataFrame()\n",
        "\n",
        "# Load geoMap data\n",
        "geomap_dfs = []\n",
        "for theme in themes:\n",
        "    for timeframe in timeframes:\n",
        "        files = glob.glob(str(data_path / theme / timeframe / 'geoMap*.csv'))\n",
        "        for file in files:\n",
        "            try:\n",
        "                df = pd.read_csv(file, skiprows=2)\n",
        "                df.columns = ['Metro', 'Interest']\n",
        "                df['Theme'] = clean_theme_name(theme)\n",
        "                df['Timeframe'] = timeframe\n",
        "                df['Interest'] = pd.to_numeric(df['Interest'], errors='coerce').fillna(0)\n",
        "                geomap_dfs.append(df)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not read {file}: {e}\")\n",
        "\n",
        "if geomap_dfs:\n",
        "    master_geomap_df = pd.concat(geomap_dfs, ignore_index=True).drop_duplicates()\n",
        "    master_geomap_df = master_geomap_df[master_geomap_df['Metro'].str.contains('Metro', na=False)]\n",
        "    master_geomap_df['Metro'] = master_geomap_df['Metro'].str.replace(' Metro', '')\n",
        "    print(\"\\nGeoMap data loaded successfully.\")\n",
        "    print(f\"Loaded {len(master_geomap_df)} rows of geomap data.\")\n",
        "else:\n",
        "    print(\"No GeoMap data loaded.\")\n",
        "    master_geomap_df = pd.DataFrame()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Campaign & Ad Group Clustering\n",
        "\n",
        "Here we'll identify themes that behave similarly to group them into effective campaigns. We'll run this analysis for each timeframe (5-year, 2-year, 1-year) to see how groupings evolve.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_clustering(timeframe, timeline_df, geomap_df):\n",
        "    \"\"\"Performs seasonal and geographic clustering for a given timeframe.\"\"\"\n",
        "    print(f\"--- Clustering Analysis for {timeframe} ---\")\n",
        "\n",
        "    # Filter data for the timeframe\n",
        "    timeline_subset = timeline_df[timeline_df['Timeframe'] == timeframe]\n",
        "    geomap_subset = geomap_df[geomap_df['Timeframe'] == timeframe]\n",
        "\n",
        "    if timeline_subset.empty or geomap_subset.empty:\n",
        "        print(f\"Not enough data for {timeframe} to perform clustering.\\\\n\")\n",
        "        return\n",
        "\n",
        "    # --- Seasonality Clustering ---\n",
        "    timeline_subset['Month'] = timeline_subset['Week'].dt.month\n",
        "    seasonal_pivot = timeline_subset.pivot_table(index='Theme', columns='Month', values='Interest', aggfunc='mean').fillna(0)\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    seasonal_scaled = scaler.fit_transform(seasonal_pivot)\n",
        "    \n",
        "    # Using k=3 as determined in the initial analysis for consistency\n",
        "    k_seasonal = 3\n",
        "    kmeans_seasonal = KMeans(n_clusters=k_seasonal, random_state=42, n_init=10)\n",
        "    seasonal_pivot['Seasonal_Cluster'] = kmeans_seasonal.fit_predict(seasonal_scaled)\n",
        "\n",
        "    # --- Geographic Clustering ---\n",
        "    geo_pivot = geomap_subset.pivot_table(index='Theme', columns='Metro', values='Interest').fillna(0)\n",
        "    geo_scaled = scaler.fit_transform(geo_pivot)\n",
        "    \n",
        "    k_geo = 3\n",
        "    kmeans_geo = KMeans(n_clusters=k_geo, random_state=42, n_init=10)\n",
        "    # Ensure geo_pivot has themes that are also in seasonal_pivot\n",
        "    geo_pivot_aligned = geo_pivot.reindex(seasonal_pivot.index).fillna(0)\n",
        "    geo_scaled_aligned = scaler.fit_transform(geo_pivot_aligned)\n",
        "    geo_pivot_aligned['Geo_Cluster'] = kmeans_geo.fit_predict(geo_scaled_aligned)\n",
        "\n",
        "    # --- Combined Analysis & Recommendation ---\n",
        "    clusters = pd.DataFrame({\n",
        "        'Seasonal_Cluster': seasonal_pivot['Seasonal_Cluster'],\n",
        "        'Geo_Cluster': geo_pivot_aligned['Geo_Cluster']\n",
        "    })\n",
        "    \n",
        "    print(\"\\\\n**Strategic Recommendation: Campaign Structure**\")\n",
        "    for i in range(k_seasonal):\n",
        "        for j in range(k_geo):\n",
        "            cluster_group = clusters[(clusters['Seasonal_Cluster'] == i) & (clusters['Geo_Cluster'] == j)]\n",
        "            if not cluster_group.empty:\n",
        "                print(f\"\\\\n*   **Campaign: Seasonal Cluster {i} / Geo Cluster {j}**\")\n",
        "                print(f\"    *   **Themes:** {', '.join(cluster_group.index)}\")\n",
        "    print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
        "\n",
        "# Run clustering for each timeframe\n",
        "if not master_timeline_df.empty and not master_geomap_df.empty:\n",
        "    for tf in timeframes:\n",
        "        perform_clustering(tf, master_timeline_df, master_geomap_df)\n",
        "else:\n",
        "    print(\"Master dataframes are empty. Skipping clustering.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Market Prioritization\n",
        "\n",
        "Here we'll identify the top 5 themes by search volume and year-over-year growth. This will be done for each timeframe to track how market priorities are shifting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_market_prioritization(timeframe, timeline_df):\n",
        "    \"\"\"Performs market prioritization analysis for a given timeframe.\"\"\"\n",
        "    print(f\"--- Market Prioritization for {timeframe} ---\")\n",
        "\n",
        "    timeline_subset = timeline_df[timeline_df['Timeframe'] == timeframe]\n",
        "    if timeline_subset.empty:\n",
        "        print(f\"No timeline data for {timeframe}.\\\\n\")\n",
        "        return\n",
        "        \n",
        "    # a) Top 5 by Volume\n",
        "    avg_volume = timeline_subset.groupby('Theme')['Interest'].mean().sort_values(ascending=False)\n",
        "    top5_volume = avg_volume.head(5)\n",
        "    print(\"\\\\n**Top 5 Themes by Average Search Volume:**\")\n",
        "    print(top5_volume.to_string())\n",
        "\n",
        "    # b) Top 5 by Growth (CAGR)\n",
        "    timeline_subset['Year'] = timeline_subset['Week'].dt.year\n",
        "    yearly_interest = timeline_subset.groupby(['Theme', 'Year'])['Interest'].mean().reset_index()\n",
        "\n",
        "    def calculate_cagr(df):\n",
        "        df = df.sort_values('Year')\n",
        "        if len(df) < 2: return np.nan\n",
        "        start_row = df.iloc[0]\n",
        "        end_row = df.iloc[-1]\n",
        "        start_value, end_value = start_row['Interest'], end_row['Interest']\n",
        "        num_years = end_row['Year'] - start_row['Year']\n",
        "        if num_years > 0 and start_value > 0:\n",
        "            return ((end_value / start_value) ** (1 / num_years)) - 1\n",
        "        return np.nan\n",
        "\n",
        "    cagr_results = yearly_interest.groupby('Theme').apply(calculate_cagr).dropna()\n",
        "    top5_growth = cagr_results.sort_values(ascending=False).head(5)\n",
        "    if not top5_growth.empty:\n",
        "        print(\"\\\\n**Top 5 Themes by Year-over-Year Growth (CAGR):**\")\n",
        "        print(top5_growth.to_string())\n",
        "    else:\n",
        "        print(\"\\\\n**Growth data not sufficient for CAGR calculation.**\")\n",
        "        \n",
        "    # Strategic Recommendation\n",
        "    print(\"\\\\n**Strategic Recommendation: Budget Allocation**\")\n",
        "    print(f\"For the **{timeframe}** perspective:\")\n",
        "    print(f\"- Focus immediate budget on high-volume themes: **{', '.join(top5_volume.index)}**.\")\n",
        "    if not top5_growth.empty:\n",
        "        print(f\"- Invest in future growth with themes like: **{', '.join(top5_growth.index)}**.\")\n",
        "    print(\"Comparing timeframes will show which high-growth themes are sustainable.\")\n",
        "    print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
        "\n",
        "# Run analysis for each timeframe\n",
        "if not master_timeline_df.empty:\n",
        "    for tf in timeframes:\n",
        "        perform_market_prioritization(tf, master_timeline_df)\n",
        "else:\n",
        "    print(\"Master timeline dataframe is empty. Skipping market prioritization.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Detailed Thematic Analysis\n",
        "\n",
        "This section provides a summary card for each theme, detailing peak seasonality and the top metro area for search interest. We'll compare these across timeframes to see how user behavior is changing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_thematic_analysis(timeframe, timeline_df, geomap_df):\n",
        "    \"\"\"Performs detailed thematic analysis for a given timeframe.\"\"\"\n",
        "    print(f\"--- Detailed Thematic Analysis for {timeframe} ---\")\n",
        "\n",
        "    timeline_subset = timeline_df[timeline_df['Timeframe'] == timeframe]\n",
        "    geomap_subset = geomap_df[geomap_df['Timeframe'] == timeframe]\n",
        "    \n",
        "    if timeline_subset.empty or geomap_subset.empty:\n",
        "        print(f\"Not enough data for {timeframe} to perform thematic analysis.\\\\n\")\n",
        "        return\n",
        "\n",
        "    all_themes = sorted(timeline_subset['Theme'].unique())\n",
        "\n",
        "    for theme in all_themes:\n",
        "        print(f\"\\\\n**Theme: {theme}**\")\n",
        "        \n",
        "        # Peak Seasonality\n",
        "        theme_timeline = timeline_subset[timeline_subset['Theme'] == theme]\n",
        "        theme_timeline['Week_of_Year'] = theme_timeline['Week'].dt.isocalendar().week\n",
        "        peak_week = theme_timeline.groupby('Week_of_Year')['Interest'].mean().idxmax()\n",
        "        \n",
        "        # Top Metro Area\n",
        "        theme_geomap = geomap_subset[geomap_subset['Theme'] == theme]\n",
        "        top_metro_name = \"N/A\"\n",
        "        if not theme_geomap.empty:\n",
        "            top_metro = theme_geomap.loc[theme_geomap['Interest'].idxmax()]\n",
        "            top_metro_name = top_metro['Metro']\n",
        "\n",
        "        print(f\"*   **Peak Seasonality:** Week {peak_week}\")\n",
        "        print(f\"*   **Top Metro Area:** {top_metro_name}\")\n",
        "        print(f\"*   **Strategic Recommendation:** For the {timeframe} view, focus ad scheduling around week {peak_week} and prioritize the {top_metro_name} metro area.\")\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
        "\n",
        "\n",
        "# Run analysis for each timeframe\n",
        "if not master_timeline_df.empty and not master_geomap_df.empty:\n",
        "    for tf in timeframes:\n",
        "        perform_thematic_analysis(tf, master_timeline_df, master_geomap_df)\n",
        "else:\n",
        "    print(\"Master dataframes are empty. Skipping thematic analysis.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Geographic Deep Dive: Top Metro Areas\n",
        "\n",
        "This section identifies the top 5 metro areas by overall search volume and the most popular themes within those metros. Comparing this across timeframes will reveal shifts in geographic market importance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_geographic_deep_dive(timeframe, geomap_df):\n",
        "    \"\"\"Performs a geographic deep dive for a given timeframe.\"\"\"\n",
        "    print(f\"--- Geographic Deep Dive for {timeframe} ---\")\n",
        "\n",
        "    geomap_subset = geomap_df[geomap_df['Timeframe'] == timeframe]\n",
        "    if geomap_subset.empty:\n",
        "        print(f\"No geomap data for {timeframe}.\\\\n\")\n",
        "        return\n",
        "\n",
        "    # a) Top 5 metro areas overall\n",
        "    metro_total_interest = geomap_subset.groupby('Metro')['Interest'].sum().sort_values(ascending=False)\n",
        "    top5_metros = metro_total_interest.head(5)\n",
        "\n",
        "    print(\"\\\\n**Top 5 Metro Areas by Overall Search Volume:**\")\n",
        "    print(top5_metros.to_string())\n",
        "\n",
        "    # b) Top themes within each top metro\n",
        "    print(\"\\\\n**Top Themes within Top 5 Metro Areas:**\")\n",
        "    for metro in top5_metros.index:\n",
        "        print(f\"\\\\n*   **Metro: {metro}**\")\n",
        "        metro_themes = geomap_subset[geomap_subset['Metro'] == metro]\n",
        "        top3_themes = metro_themes.sort_values('Interest', ascending=False).head(3)\n",
        "        for _, row in top3_themes.iterrows():\n",
        "            print(f\"    *   {row['Theme']} (Interest: {row['Interest']})\")\n",
        "            \n",
        "    print(\"\\\\n**Strategic Recommendation: Geo-Targeted Ad Copy**\")\n",
        "    print(f\"Based on the {timeframe} data, tailor ad copy for these top metros. For example, for users in **{top5_metros.index[0]}**, focus on themes like **{geomap_subset[geomap_subset['Metro'] == top5_metros.index[0]].sort_values('Interest', ascending=False).iloc[0]['Theme']}**.\")\n",
        "    print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
        "\n",
        "# Run analysis for each timeframe\n",
        "if not master_geomap_df.empty:\n",
        "    for tf in timeframes:\n",
        "        perform_geographic_deep_dive(tf, master_geomap_df)\n",
        "else:\n",
        "    print(\"Master geomap dataframe is empty. Skipping geographic deep dive.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Final Report Generation\n",
        "\n",
        "This final step will consolidate all the analysis and recommendations into a clean, well-structured markdown report.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Analysis notebook complete. Ready to generate report.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
